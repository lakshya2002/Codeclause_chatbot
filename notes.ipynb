{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline is nothing but a set of algorithms to be used to train your model.\n",
    "- Rasa NLU has two widely used pipelines called <b>spacy_sklearn</b> and <b>tensorflow_embedding</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow_embedding pipeline doesnâ€™t make use of any pre-trained \n",
    "word vectors like spacy_sklearn, but it adjusts itself as per our own \n",
    "provided dataset.\n",
    "- The good thing about tensorflow_embedding pipeline is that our word \n",
    "vectors will be as per our domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lemmatization is the algorithmic \n",
    "process of determining the lemma of a word based on its intended meaning\n",
    "-  It tries its best to \n",
    "remove inflectional endings only and return the dictionary form of a \n",
    "word, known as the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_set = []\n",
    "for i in range(len(sentences)):\n",
    "    bag_of_words = tfidf_features[i].tolist()\n",
    "    intent_label = [1 if tag == labels[i] else 0 for tag in classes]\n",
    "    training_set.append([bag_of_words, intent_label])\n",
    "In this step, we create the training set for intent classification. We loop through each sentence in sentences and perform the following:\n",
    "\n",
    "Convert the TF-IDF features for the current sentence into a list and store it in the variable bag_of_words.\n",
    "Create the intent label as a one-hot encoded list. If the intent tag matches the current sentence's label, we set the corresponding element to 1; otherwise, we set it to 0.\n",
    "Append the pair of bag-of-words representation (bag_of_words) and intent label (intent_label) to the training_set list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the intent labels are stored in a 0 or 1 array. In the context of intent classification, we often use a one-hot encoding scheme to represent the intent labels.\n",
    "\n",
    "In the code provided, intent_label is a list that represents the one-hot encoding for the current intent. For example, if we have three intent classes, let's say [\"greeting\", \"goodbye\", \"thanks\"], and the current pattern is associated with the \"thanks\" intent, then the intent_label for that pattern will be [0, 0, 1].\n",
    "\n",
    "Here's an explanation of the one-hot encoding:\n",
    "\n",
    "Each element in the intent_label list corresponds to a specific intent class in the order they appear in the classes list.\n",
    "The value 1 is placed in the index position that corresponds to the current intent class, and all other positions are set to 0.\n",
    "For instance, if classes is [\"greeting\", \"goodbye\", \"thanks\"], and the current intent is \"thanks,\" the one-hot encoding for that intent will be [0, 0, 1].\n",
    "\n",
    "Using one-hot encoding helps the machine learning model to understand the categorical nature of the intent labels, as each label is represented by a unique binary pattern. This makes it easier for the model to learn and make predictions during the training process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the stochastic gradient descent (SGD) optimizer with Nesterov accelerated gradient\n",
    "# lr: Learning rate (controls the step size in the gradient descent)\n",
    "# decay: Learning rate decay over each update\n",
    "# momentum: Controls how much of the previous gradient direction to keep (helps to converge faster)\n",
    "# nesterov: Whether to use Nesterov momentum (an improvement over traditional momentum)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model using categorical cross-entropy as the loss function and the SGD optimizer\n",
    "# 'categorical_crossentropy': This is commonly used for multi-class classification tasks\n",
    "# The model will be trained to minimize the cross-entropy between predicted and true labels\n",
    "# optimizer: SGD with Nesterov accelerated gradient\n",
    "# metrics: List of metrics to be evaluated during training and testing, here we use 'accuracy'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
