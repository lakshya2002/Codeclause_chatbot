{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline is nothing but a set of algorithms to be used to train your model.\n",
    "- Rasa NLU has two widely used pipelines called <b>spacy_sklearn</b> and <b>tensorflow_embedding</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensorflow_embedding pipeline doesn’t make use of any pre-trained \n",
    "word vectors like spacy_sklearn, but it adjusts itself as per our own \n",
    "provided dataset.\n",
    "- The good thing about tensorflow_embedding pipeline is that our word \n",
    "vectors will be as per our domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lemmatization is the algorithmic \n",
    "process of determining the lemma of a word based on its intended meaning\n",
    "-  It tries its best to \n",
    "remove inflectional endings only and return the dictionary form of a \n",
    "word, known as the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_set = []\n",
    "for i in range(len(sentences)):\n",
    "    bag_of_words = tfidf_features[i].tolist()\n",
    "    intent_label = [1 if tag == labels[i] else 0 for tag in classes]\n",
    "    training_set.append([bag_of_words, intent_label])\n",
    "In this step, we create the training set for intent classification. We loop through each sentence in sentences and perform the following:\n",
    "\n",
    "Convert the TF-IDF features for the current sentence into a list and store it in the variable bag_of_words.\n",
    "Create the intent label as a one-hot encoded list. If the intent tag matches the current sentence's label, we set the corresponding element to 1; otherwise, we set it to 0.\n",
    "Append the pair of bag-of-words representation (bag_of_words) and intent label (intent_label) to the training_set list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the intent labels are stored in a 0 or 1 array. In the context of intent classification, we often use a one-hot encoding scheme to represent the intent labels.\n",
    "\n",
    "In the code provided, intent_label is a list that represents the one-hot encoding for the current intent. For example, if we have three intent classes, let's say [\"greeting\", \"goodbye\", \"thanks\"], and the current pattern is associated with the \"thanks\" intent, then the intent_label for that pattern will be [0, 0, 1].\n",
    "\n",
    "Here's an explanation of the one-hot encoding:\n",
    "\n",
    "Each element in the intent_label list corresponds to a specific intent class in the order they appear in the classes list.\n",
    "The value 1 is placed in the index position that corresponds to the current intent class, and all other positions are set to 0.\n",
    "For instance, if classes is [\"greeting\", \"goodbye\", \"thanks\"], and the current intent is \"thanks,\" the one-hot encoding for that intent will be [0, 0, 1].\n",
    "\n",
    "Using one-hot encoding helps the machine learning model to understand the categorical nature of the intent labels, as each label is represented by a unique binary pattern. This makes it easier for the model to learn and make predictions during the training process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the stochastic gradient descent (SGD) optimizer with Nesterov accelerated gradient\n",
    "# lr: Learning rate (controls the step size in the gradient descent)\n",
    "# decay: Learning rate decay over each update\n",
    "# momentum: Controls how much of the previous gradient direction to keep (helps to converge faster)\n",
    "# nesterov: Whether to use Nesterov momentum (an improvement over traditional momentum)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model using categorical cross-entropy as the loss function and the SGD optimizer\n",
    "# 'categorical_crossentropy': This is commonly used for multi-class classification tasks\n",
    "# The model will be trained to minimize the cross-entropy between predicted and true labels\n",
    "# optimizer: SGD with Nesterov accelerated gradient\n",
    "# metrics: List of metrics to be evaluated during training and testing, here we use 'accuracy'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Activation layer</h3></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- About Activation layer:\n",
    "\n",
    "An activation function is a crucial component of artificial neural networks. It introduces non-linearity to the neural network, allowing it to learn complex patterns and make more accurate predictions. The activation function is applied to the output of each neuron in a neural network, transforming the neuron's input into its output.\n",
    "\n",
    "The purpose of an activation function is twofold:\n",
    "\n",
    "- Introducing Non-Linearity: Without a non-linear activation function, no matter how many layers we add, the neural network would behave like a single-layer perceptron, as linear operations are simply a composition of other linear operations. With non-linearity introduced by activation functions, the neural network can approximate complex, non-linear relationships between input and output data.\n",
    "\n",
    "- Decision Making: The activation function determines whether a neuron should be activated (output a non-zero value) or not (output zero) based on its input. This behavior is akin to how real neurons in the human brain \"fire\" (activate) or not, depending on the signals they receive.\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "- Sigmoid: It squashes the output between 0 and 1, making it suitable for binary classification problems. However, it suffers from the vanishing gradient problem, which can lead to slower convergence during training.\n",
    "\n",
    "- ReLU (Rectified Linear Unit): It returns the input if it is positive, and zero otherwise. It has become popular due to its simplicity and effectiveness in mitigating the vanishing gradient problem.\n",
    "\n",
    "- Tanh (Hyperbolic Tangent): Similar to the sigmoid function, but it maps the output between -1 and 1, making it more centered and often leading to faster convergence.\n",
    "\n",
    "- Softmax: Used in the output layer for multi-class classification problems, it normalizes the output probabilities, ensuring they sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>ReLU and Softmax in detail</h3></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReLU (Rectified Linear Unit):\n",
    "\n",
    "ReLU is one of the most widely used activation functions in deep learning. It introduces non-linearity to the neural network, allowing it to learn complex patterns in the data. The ReLU function is defined as follows:\n",
    "f(x) = max(0, x)\n",
    "\n",
    "In other words, if the input value (x) is greater than zero, the output of the ReLU function will be the input value itself. If the input value is less than or equal to zero, the output will be zero. The ReLU function has a simple and computationally efficient implementation, and it is computationally cheaper than other activation functions like sigmoid and tanh.\n",
    "\n",
    "Advantages of ReLU:\n",
    "\n",
    "- It helps overcome the vanishing gradient problem, which can occur in networks that use sigmoid or tanh activation functions, by allowing gradients to flow more easily during backpropagation.\n",
    "ReLU is computationally efficient and easy to implement, making it well-suited for deep learning architectures.\n",
    "However, ReLU has a limitation called the \"dying ReLU\" problem. Neurons that have a negative output for all inputs during training are essentially \"dead\" because their gradients are always zero. This can happen when a large gradient flows through the neuron during training, causing the weights to update in such a way that the neuron is always inactive. To mitigate this issue, variants of ReLU, such as Leaky ReLU and Parametric ReLU, have been introduced.\n",
    "\n",
    "Softmax:\n",
    "- Softmax is an activation function used specifically in the output layer of neural networks for multiclass classification problems. It converts raw scores (logits) into a probability distribution over multiple classes. The Softmax function is defined as follows:\n",
    "f(xi) = exp(xi) / Σ exp(xj) for all j in classes\n",
    "\n",
    "- Where xi is the raw score of the class i, and the denominator is the sum of the exponential of all raw scores in all classes. The output of the Softmax function for each class is a probability value between 0 and 1, and the sum of all class probabilities will be equal to 1.\n",
    "\n",
    "- Softmax is commonly used in conjunction with the categorical cross-entropy loss function, which measures the difference between predicted probabilities and true labels. During training, the model aims to minimize the cross-entropy loss to improve its performance on the classification task.\n",
    "\n",
    "Advantages of Softmax:\n",
    "\n",
    "- Softmax provides a probabilistic interpretation of the model's predictions, making it easier to interpret the output as class probabilities.\n",
    "It ensures that the model's predictions are mutually exclusive (i.e., a sample belongs to only one class).\n",
    "Both ReLU and Softmax activation functions play crucial roles in the success of deep learning models by introducing non-linearity and converting raw scores into meaningful outputs, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><h3>Stochastic Gradient Descent (SGD)</h3></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic Gradient Descent (SGD) is an optimization algorithm commonly used in training neural networks. It is a variant of the gradient descent algorithm that aims to find the optimal set of weights and biases for the neural network by minimizing the loss function.\n",
    "\n",
    "- The main idea behind SGD is to update the model's parameters (weights and biases) after each training sample rather than after the entire dataset. This introduces randomness into the updates, making the optimization process stochastic.\n",
    "\n",
    "Here's how the SGD algorithm works:\n",
    "\n",
    "Initialize the model's parameters with random values.\n",
    "\n",
    "Shuffle the training data randomly.\n",
    "\n",
    "For each training sample (or a small batch of samples, in the case of mini-batch SGD):\n",
    "a. Compute the gradient of the loss function with respect to each parameter using the current training sample.\n",
    "b. Update the model's parameters in the opposite direction of the gradient to minimize the loss function. The update is performed with a learning rate, which controls the step size of the parameter update.\n",
    "\n",
    "Repeat the process for a fixed number of epochs (complete passes through the entire dataset) or until convergence.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "- Stochastic Gradient Descent is computationally more efficient than traditional gradient descent because it updates the parameters after each sample, avoiding the need to calculate gradients for the entire dataset.\n",
    "- The use of small batches (mini-batches) in mini-batch SGD further enhances computational efficiency and allows for parallel processing in hardware like GPUs.\n",
    "SGD introduces noise into the optimization process, which can help the model escape local minima and converge to a better solution.\n",
    "- However, the randomness in updates can lead to more oscillations during training, which can make convergence slower compared to other optimization algorithms.\n",
    "To mitigate the oscillations, learning rate scheduling or momentum techniques are often used with SGD.\n",
    "- In summary, Stochastic Gradient Descent is an optimization algorithm used to train neural networks. It updates the model's parameters after processing each training sample or batch, which makes it computationally efficient and helps in escaping local minima. However, it can be sensitive to learning rate choices and might exhibit slower convergence compared to other optimization algorithms. Various adaptations and enhancements, such as learning rate schedules and momentum, are commonly used with SGD to improve its performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
